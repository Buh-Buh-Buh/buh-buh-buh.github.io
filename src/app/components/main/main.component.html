<div fxLayout="column" fxLayoutAlign="center center" width="100%" class="main">
  <mat-card class="mat-elevation-z5 info-card">
    <mat-card-content fxLayout="row wrap" fxLayoutAlign="center center">
      <a href="assets/images/infographic.png">
        <div class="infographic-container">
          <img src="assets/images/infographic.png" class="infographic" />
        </div>
      </a>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Problem Background</mat-card-title>
    <mat-card-content>
      With the explosion of online media outlets, fake news has become an increasingly pervasive issue in today's global climate to the point where
      widespread misinformation on social media largely influenced the 2016 election.<span [innerHTML]="cite('Aldwairi')"></span>
      In a 2016 study, 64% of Americans felt that made-up news has caused a great deal of confusion about basic facts of current events.<span [innerHTML]="cite('Barthel')"></span>
      The problem is only getting worse with 38% of Americans finding it harder to identify what is true or false.<span [innerHTML]="cite('Mitchell')"></span>
      Humans are surprisingly bad at detecting lies in text—we are only 4% better than random chance at detecting fake news!<span [innerHTML]="cite('Conroy')"></span>
      With fake news becoming such an important and critical issue to the health and safety of the public and in all facets of life, we wish to take a stab at alleviating and reducing some of the negative impacts of fake news.<br>

      The fake news or "deception detection" problem is not new to the natural language processing community. As early as 2011, people were already attempting to detect fake news or deception in review opinions.<span [innerHTML]="cite('Wang')"></span>
      Since then, stylometric, semi-supervised learning, and linguistic approaches have been used to detect deceptive text on crowdsource-generated datasets<span [innerHTML]="cite('Wang')"></span>
      with linguistic and network approaches being the most successful.<span [innerHTML]="cite('Conroy')"></span>
      However, prior to 2017, most machine learning approaches were limited by the lack of a large amount of labeled real world data as the best existing data at the time was small and crowd generated.<span [innerHTML]="cite('Wang')"></span>
      With better data in recent years, people have used majority models, support vector machine classifiers (SVM), bi-directional long short-term memory networks (Bi-LSTM), and convolutional neural networks (CNN) with varying success.<span [innerHTML]="cite('Wang')"></span>
      <br>

      However, current models and practices are limited in that they need to be retrained over time with more recent data to accurately detect fake news as reporting styles change over time.
      Additionally, the success of current machine learning approaches are extremely dependent on the training data set used. How realistic, large, varied, and well-labeled the dataset will all influence
      the performance of our ML model. Another consideration is that there is little consensus on what constitutes fake news; usually the labeling of the data set determines this.
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Proposal</mat-card-title>
    <mat-card-content>
      We will parse the titles and content of various labeled articles to train a model to decide whether an article contains fake news or not.

      Our approach will attempt to rate articles on a scale from fake to real (based on a confidence interval) rather than a binary decision as
      we believe a continuous scale will help people make their own final judgement based on how poorly/well an article is rated when run through the trained model.

      Our goal is to help prevent the spread of misinformation and fake news by using our model to judge an article's validity.
      To do this, we hope to create a chrome extension to make this model easily accessible to the common user.<br>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Methods</mat-card-title>
    <mat-card-content fxLayout="column" fxLayoutAlign="center start">
      <mat-card-title>Initially Planned Methodology</mat-card-title>
      <p>
        To clean our data, we plan on preprocessing the input data to remove various punctuations, which affect the performance of some text analysis algorithms.
        Based on the success (99.4% accuracy) of a similar project in 2018,<span [innerHTML]="cite('Aldwairi')"></span> we also plan on feature engineering new features for our models to train with.
        These features may include various properties about the article titles, such as all caps, containing many ? and ! marks, and text-title keyword matching.
        We plan on testing models with and without these features to determine if they provide a real benefit or cause overfitting.
        Since it has been successfully utilized in the past,<span [innerHTML]="cite('Conroy')"></span><span [innerHTML]="cite('Zhou')"></span>
        we will use bag-of-words and vectorization to create features based on the text.
        We will then use PCA to determine what features will be the most relevant for the algorithms.
        We plan to explore various forms of clustering, including expectation-maximization, hierarchical clustering, and k-means to find correlation between clusters of words/other features. We will measure performance based on the best F-measure/F1-score and accuracy.
        We then plan to train supervised models based on the most relevant features to detect fake news.
        For supervised learning, we plan on exploring CNNs, SVMs, and LSTMs.
      </p>
      <a href="assets/images/data-diagram.png" fxFlexAlign="center" style="margin: 1em 0;">
        <div class="infographic-container mat-elevation-z2">
          <img src="assets/images/data-diagram.png" class="infographic" height="491" width="834" />
        </div>
      </a>
      <mat-card-title>Data Preprocessing and Representation</mat-card-title>
      <p>
        To clean our data, we removed all numbers and hyperlinks, which almost halved our feature space. This will be extremely helpful for the efficiency of training and the accuracy for all of our models.
        We also removed all English stop words (which are common words that don't have a significant contribution to the meaning of an article).<br>
        By using a TF-IDF vectorization, a variation of bag-of-words, we were able to account for the information gain from each word in the article. We dropped all features that appeared less than 5 times in the entire training dataset because they don't contribute meaning to the articles.
        The use of TF-IDF also helped us remove all punctuations.
      </p>
      <mat-card-title>Unsupervised Learning</mat-card-title>
      <p>
        In order to reduce the dimensionality of our data, we performed Latent Semantic Analysis (LSA) on our dataset.
        We used LSA (also known as truncated SVD) instead of Principal Component Analysis (PCA) because of two main reasons:
      </p>
      <ol style="margin-top: 0;">
        <li>
          LSA has been known to perform well in natural language processing (NLP) applications, performing especially well on sparse datasets, like our bag of words and TF-IDF vectorization as it does not center the data like PCA.
        </li>
        <li>
          With the size of our dataset, speed matters and LSA is significantly faster than doing a full PCA.
        </li>
      </ol>
      <p>
        When doing LSA, the number of components is very important as more components can lead to more relevant and specific learning but can also lead to overfitting. On the contrary, less components can lead to better generalization outside of the training set.
        Based on a meta-studies of many natural language processing studies utilizing LSA, there is a "island of stability" of around 300 to 500 components with 300 components being a good choice for around thousands to tens of thousands of documents and 400 for millions of documents. <span [innerHTML]="cite('Bradford')"></span>
        Thus, based on the literature, we decided to explore LSA with around 300 components, trying values from 100-500 in systematic intervals; however, the variance recovered was always relatively low (around 15 - 30%). This may be due to the fact that bag of words and TF-IDF generates datasets with such large dimensionality.
        After performing LSA on our dataset, we analyzed the first few principal components to see which features contained the most variance in our dataset. Words such as "trump", "clinton", "senate", "house", "left", "news", and "korea" showed up frequently in the first few components after LSA, showing the political nature of our dataset.

      </p>
      <p>
        After performing LSA, we attempted various types of clustering on our reduced dimensionality dataset.
        We tried:
      </p>
      <ul style="margin-top: 0;">
        <li> K-Means clustering </li>
        <li> Gaussian Mixture Models (GMM) </li>
        <li> DBSCAN </li>
      </ul>
      <p>
        Utilizing the elbow method for determining the optimal amount of clusters for K-Means and GMM, our clustering results turned out to be very poor with K-means with as much as 100 clusters only having a homogeneity score of 0.45 relative to the ground truth.
        Evaluating other clustering metrics such as the F-1 score and accuracy relative to the ground truth also showed similar results and conclusions of poor clustering performance.
        We believe that the poor clustering result is largely due to the fact that there are so many features (even after LSA) with a high degree of overlap between true and false articles.
        Additionally, when we tried DBSCAN, with our large dataset, we ran into memory errors on Colab even when we reduced the feature space even more and tried different, more memory-efficienct DBSCAN algorithms.
      </p>
      <div fxLayout="row" fxLayoutAlign="space-around" fxFlexAlign="center">
        <a href="assets/images/kmeans-loss.png" fxFlexAlign="center" style="margin: 1em 0; width: 32%;">
          <div class="infographic-container mat-elevation-z2">
            <img src="assets/images/kmeans-loss.png" class="infographic"/>
          </div>
        </a>
        <a href="assets/images/gmm-clustering.png" fxFlexAlign="center" style="margin: 1em 0; width: 60%;">
          <div class="infographic-container mat-elevation-z2">
            <img src="assets/images/gmm-clustering.png" class="infographic"/>
          </div>
        </a>
      </div>
      <mat-card-title>Supervised Learning</mat-card-title>
      <p>
        Since our unsupervised clustering performed poorly, we decided to take some early strides into supervised learning.
        The supervised model works by first preprocessing the data using TF-IDF. This is done by using a Keras Preprocessing library called Token.
        In essence, the Tokenizer takes the entire data sets of body paragraphs and creates a vocabulary of every word based on the inverse of the frequency.
        Then, once this dictionary is generated it can tokenize any string by replacing each string with a number based on how common it was in the initial entire data set.<br>

        From there we this matrix along with its labels of 0’s or 1’s into a three-layer neural net with 16 nodes each.
        We decided to use a sigmoid function to represent the outputs since we believe grading the likelihood that a new article is fake should be continuous function rather than a discrete number being either 0 or 1.
        We ran on 5 epochs as this was the amount of training time that maximized the validation accuracy. With this method we were able to achieve around 98.4% accuracy on the validation set.
      </p>
      <a href="assets/images/nn-accuracy-vs-epochs.png" fxFlexAlign="center" style="margin: 1em 0;">
        <div class="infographic-container mat-elevation-z2">
          <img src="assets/images/nn-accuracy-vs-epochs.png" class="infographic"/>
        </div>
      </a>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Results</mat-card-title>
    <mat-card-content>
      <mat-card-title>Initially Expected Results</mat-card-title>
      <p>
        The midterm evaluation will be based on the existence of an unsupervised clustering model.
        The parsing of the input data will be based on some NLP model such as bag of words or word embedding to help best store key features of the dataset.
        We plan on using the clustering results to guide our feature selection during the supervised learning phases of the project.<br>
        The final evaluation will be on the accuracy of fake news detection on recent news articles. We plan to compare the results of the learnings from the different models and methods we try.<br>
        If time permits, we hope to have a functioning Google Chrome extension that will allow users to obtain a rating (on a scale) of the accuracy of an article they are currently reading.
      </p>
      <mat-card-title class="mt-2">Midterm Results</mat-card-title>
      <p>
        At the time of the midterm, we have successfully cleaned and pre-processed our data, choosing a bag of words and TF-IDF vectorization of our articles to represent our data.
        We have sucessfully utilized LSA, a popular dimensionality reduction method for NLP applications, to reduce the dimensionality of our dataset from tens of thousands to a few hundred.
        Given this, we have tried various unsupervised clustering approaches such as K-Means, GMM, and DBSCAN with varying amounts of success, however we expect to see better results with supervised training given the nature of our NLP application.
        With some early strides into utilizing supervised learning, we were able to train a 3 layered dense neural network over 5 epochs to achieve an impressive accuracy of over 97%.<br>
        After the midterm, we plan on exploring more supervised learning methods such as support vector machine (SVM) and decision trees in addition to improving on our neural network model by fine tuning the hyperparameters.
        With a good accuracy, we hope to create a Google Chrome extention to give end-users easy access to the fake-news detection ability of our model as previously discussed.
      </p>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Discussion</mat-card-title>
    <mat-card-content>
      <mat-card-title>Initial Goals</mat-card-title>
      <p>
        The outcome we hope to achieve is one where our model is able to accurately and reliabily detect the validity of the news articles as they are released in real-time.
        We hope that our project will be able to benefit people all around the world who are affected by the fake news that plagues our internet and media consumption today.
        We believe that knowing the validity (or at least, a general idea of the validity) of articles on the internet will greatly affect the
        quality of the knowledge people gain each day about the state of the world we live in, which we believe is very important, especially in today's climate.
        Additionally, we believe a Google Chrome extension would be critical in increasing the utility and reach of our model as it would allow users to convieniently get an idea of how fake
        the current articles they have pulled up are. We believe that even if our model is not perfectly accurate, the presence of such an extension in an individual's browser would encourage them
        to think skeptically about the information they are taking in, promoting the spread of critical thinking and discouraging the spread of additional fake information.
        Our hope is that our project will be a step towards lessening the spread of misinformation in our media today.
      </p>

      <mat-card-title class="mt-2">Planned Risks</mat-card-title>
      <p>
        We have identified that there may be issues with racism/sexism when taking into account the author of an article in addition to misclassifying true articles as fake news or vice versa.
        We have realized that the success of our model and project is largely dependent on the quality of the labeled data set. Whether or not the data set is large enough, varied enough, representative of real
        world articles, and correctly labeled will be a critical determining factor in the performance of our model in evaluating real world articles outside of the labeled training data set.
      </p>

      <mat-card-title class="mt-2">Planned Costs</mat-card-title>
      <p>
        In terms of monetary cost, we do not believe that any significant funding will be needed with our current hardware available and the relatively low cost and availability of cloud computing.
        Regarding time, we believe it will mostly depend on how long it takes us to test different models and train the model as we do not expect training the model to take that much time with current cloud computing capabilities and hardware. We are also using free, labeled datasets, so the main data-related time costs would be finding more datasets if necessary.
      </p>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>References</mat-card-title>
    <mat-card-content>
      <ul class="bib" ng-model="bibliography">
        <li>
          Dataset: <a href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/discussion">Kaggle - Fake/Real News Dataset</a>
        </li>
        <li *ngFor="let reference of references; let i = index" [attr.data-index]="i">
          <a href={{reference.link}} id="ref{{i+1}}">[{{i+1}}]</a> {{reference.text}}
        <li>
      </ul>
    </mat-card-content>
  </mat-card>
</div>
