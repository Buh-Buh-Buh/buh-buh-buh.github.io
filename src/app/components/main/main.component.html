<div fxLayout="column" fxLayoutAlign="center center" width="100%" class="main">
  <mat-card class="mat-elevation-z5 info-card">
    <mat-card-content fxLayout="row wrap" fxLayoutAlign="center center">
      <a href="assets/images/infographic.png">
        <div class="infographic-container">
          <img src="assets/images/infographic.png" class="infographic" />
        </div>
      </a>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Problem Background</mat-card-title>
    <mat-card-content>
      With the explosion of online media outlets, fake news has become an increasingly pervasive issue in today's global climate to the point where
      widespread misinformation on social media largely influenced the 2016 election.<span [innerHTML]="cite('Aldwairi')"></span>
      In a 2016 study, 64% of Americans felt that made-up news has caused a great deal of confusion about basic facts of current events.<span [innerHTML]="cite('Barthel')"></span>
      The problem is only getting worse with 38% of Americans finding it harder to identify what is true or false.<span [innerHTML]="cite('Mitchell')"></span>
      Humans are surprisingly bad at detecting lies in textâ€”we are only 4% better than random chance at detecting fake news!<span [innerHTML]="cite('Conroy')"></span>
      With fake news becoming such an important and critical issue to the health and safety of the public and in all facets of life, we wish to take a stab at alleviating and reducing some of the negative impacts of fake news.<br>

      The fake news or "deception detection" problem is not new to the natural language processing community. As early as 2011, people were already attempting to detect fake news or deception in review opinions.<span [innerHTML]="cite('Wang')"></span>
      Since then, stylometric, semi-supervised learning, and linguistic approaches have been used to detect deceptive text on crowdsource-generated datasets<span [innerHTML]="cite('Wang')"></span>
      with linguistic and network approaches being the most successful.<span [innerHTML]="cite('Conroy')"></span>
      However, prior to 2017, most machine learning approaches were limited by the lack of a large amount of labeled real world data as the best existing data at the time was small and crowd generated.<span [innerHTML]="cite('Wang')"></span>
      With better data in recent years, people have used majority models, support vector machine classifiers (SVM), bi-directional long short-term memory networks (Bi-LSTM), and convolutional neural networks (CNN) with varying success.<span [innerHTML]="cite('Wang')"></span>
      <br>

      However, current models and practices are limited in that they need to be retrained over time with more recent data to accurately detect fake news as reporting styles change over time.
      Additionally, the success of current machine learning approaches are extremely dependent on the training data set used. How realistic, large, varied, and well-labeled the dataset will all influence
      the performance of our ML model. Another consideration is that there is little consensus on what constitutes fake news; usually the labeling of the data set determines this.
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Proposal</mat-card-title>
    <mat-card-content>
      We will parse the titles and content of various labeled articles to train a model to decide whether an article contains fake news or not.

      Our approach will attempt to rate articles on a scale from fake to real (based on a confidence interval) rather than a binary decision as
      we believe a continuous scale will help people make their own final judgement based on how poorly/well an article is rated when run through the trained model.

      Our goal is to help prevent the spread of misinformation and fake news by using our model to judge an article's validity.
      To do this, we hope to create a chrome extension to make this model easily accessible to the common user.<br>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Methods</mat-card-title>
    <mat-card-content fxLayout="column" fxLayoutAlign="center start">
      <p>
        To clean our data, we plan on preprocessing the input data to remove various punctuations, which affect the performance of some text analysis algorithms.
        Based on the success (99.4% accuracy) of a similar project in 2018,<span [innerHTML]="cite('Aldwairi')"></span> we also plan on feature engineering new features for our models to train with.
        These features may include various properties about the article titles, such as all caps, containing many ? and ! marks, and text-title keyword matching.
        We plan on testing models with and without these features to determine if they provide a real benefit or cause overfitting.
        Since it has been successfully utilized in the past,<span [innerHTML]="cite('Conroy')"></span><span [innerHTML]="cite('Zhou')"></span>
        we will use bag-of-words and vectorization to create features based on the text.
        We will then use PCA to determine what features will be the most relevant for the algorithms.
        We plan to explore various forms of clustering, including expectation-maximization, hierarchical clustering, and k-means to find correlation between clusters of words/other features. We will measure performance based on the best F-measure/F1-score and accuracy.
        We then plan to train supervised models based on the most relevant features to detect fake news.
        For supervised learning, we plan on exploring CNNs, SVMs, and LSTMs.
      </p>
      <a href="assets/images/data-diagram.png" fxFlexAlign="center" style="margin-top: 1em;">
        <div class="infographic-container mat-elevation-z2 mat-pt-5">
          <img src="assets/images/data-diagram.png" class="infographic" />
        </div>
      </a>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Expected Results</mat-card-title>
    <mat-card-content>
      The midterm evaluation will be based on the existence of an unsupervised clustering model.
      The parsing of the input data will be based on some NLP model such as bag of words or word embedding to help best store key features of the dataset.
      We plan on using the clustering results to guide our feature selection during the supervised learning phases of the project.<br>
      The final evaluation will be on the accuracy of fake news detection on recent news articles. We plan to compare the results of the learnings from the different models and methods we try.<br>
      If time permits, we hope to have a functioning Google Chrome extension that will allow users to obtain a rating (on a scale) of the accuracy of an article they are currently reading.
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>Discussion</mat-card-title>
    <mat-card-content>
      <p>
        The outcome we hope to achieve is one where our model is able to accurately and reliabily detect the validity of the news articles as they are released in real-time.
        We hope that our project will be able to benefit people all around the world who are affected by the fake news that plagues our internet and media consumption today.
        We believe that knowing the validity (or at least, a general idea of the validity) of articles on the internet will greatly affect the
        quality of the knowledge people gain each day about the state of the world we live in, which we believe is very important, especially in today's climate.
        Additionally, we believe a Google Chrome extension would be critical in increasing the utility and reach of our model as it would allow users to convieniently get an idea of how fake
        the current articles they have pulled up are. We believe that even if our model is not perfectly accurate, the presence of such an extension in an individual's browser would encourage them
        to think skeptically about the information they are taking in, promoting the spread of critical thinking and discouraging the spread of additional fake information.
        Our hope is that our project will be a step towards lessening the spread of misinformation in our media today.
      </p>

      <mat-card-title class="mt-2">Risks</mat-card-title>
      <p>
        We have identified that there may be issues with racism/sexism when taking into account the author of an article in addition to misclassifying true articles as fake news or vice versa.
        We have realized that the success of our model and project is largely dependent on the quality of the labeled data set. Whether or not the data set is large enough, varied enough, representative of real
        world articles, and correctly labeled will be a critical determining factor in the performance of our model in evaluating real world articles outside of the labeled training data set.
      </p>

      <mat-card-title class="mt-2">Costs</mat-card-title>
      <p>
        In terms of monetary cost, we do not believe that any significant funding will be needed with our current hardware available and the relatively low cost and availability of cloud computing.
        Regarding time, we believe it will mostly depend on how long it takes us to test different models and train the model as we do not expect training the model to take that much time with current cloud computing capabilities and hardware. We are also using free, labeled datasets, so the main data-related time costs would be finding more datasets if necessary.
      </p>
    </mat-card-content>
  </mat-card>
  <mat-card class="mat-elevation-z5">
    <mat-card-title>References</mat-card-title>
    <mat-card-content>
      <ul class="bib" ng-model="bibliography">
        <li>
          Dataset: <a href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/discussion">Kaggle - Fake/Real News Dataset</a>
        </li>
        <li *ngFor="let reference of references; let i = index" [attr.data-index]="i">
          <a href={{reference.link}} id="ref{{i+1}}">[{{i+1}}]</a> {{reference.text}}
        <li>
      </ul>
    </mat-card-content>
  </mat-card>
</div>
